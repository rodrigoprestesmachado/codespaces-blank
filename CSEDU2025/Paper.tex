\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{algorithm2e}
\usepackage[bottom]{footmisc}
% Please add other packages that you may need BEFORE the SCITEPRESS.sty package.
\usepackage{natbib}
\usepackage{multirow}
\usepackage{SCITEPRESS}
\usepackage{arydshln}

\begin{document}

\title{
  Analyzing Student Use of Spacing and Interleaving Strategies in Interactions
  with GenAI-Powered Chatbots in Programming Courses
}

\author{\authorname{Rodrigo Prestes Machado\sup{1}\orcidAuthor{0000-0003-0428-6387},
Carlos Alario Hoyos\sup{2}\orcidAuthor{0000-0002-3082-0814},
Patricia Callejo Pinardo\sup{2}\orcidAuthor{0000-0001-6124-6213},
Iria Estévez-Ayres\sup{2}\orcidAuthor{0000-0002-1047-5398},
Carlos Delgado Kloos\sup{2}\orcidAuthor{0000-0003-4093-3705}}
\affiliation{\sup{1}Department of Informatics, Instituto Federal de Educação,
Ciência e Tecnologia, Porto Alegre, Brazil}
\affiliation{\sup{2}Department of Telematics Engineering, Universidad Carlos III
de Madrid, Leganés (Madrid), Spain}
\email{\ rodrigo.prestes@poa.ifrs.edu.br, \{calario,pcallejo, ayres,cdk\}@it.uc3m.es}
}

\keywords{
  Programming, Generative Artificial Intelligence, Chatbots, Spacing,
  Interleaving and Metacognition
}

\abstract{
This study examines the interactions of programming students with a chatbot
powered by OpenAI's GPT-3.5 and enhanced with the Retrieval-Augmented Generation
(RAG) technique, within the context of a Java programming course. It
investigates how students use the bot while employing two metacognitive
strategies: interleaving and spacing. Using the logs of student interactions
with the bot, the methodology involved classifying and analyzing student prompts
into eight distinct categories. The findings reveal that while most interactions
emphasize active learning strategies, students demonstrate limited conscious
application of interleaving and spacing. This study underscores the need for
more guidance on the strategic use of AI tools to improve learning outcomes.
}

\twocolumn\maketitle\normalsize\setcounter{footnote}{0}

\section{\uppercase{Introduction}}
\label{sec:introduction}

Recent advances in Generative Artificial Intelligence (GenAI) have created
new opportunities in both professional programming \citep{Peng23}
\citep{Pandey24} and education \citep{Puryear22}. In programming courses,
students can employ GenAI tools to improve their understanding, receive
personalized feedback, and access detailed explanations. For example, GitHub
Copilot, a tool that is integrated into development environments, assists in
providing real-time suggestions and accelerating code writing, which could help
streamline the learning process \citep{Denny23}. Educational chatbots, on the
other hand, try to guide students through coding challenges, answer questions,
and try to foster a deeper understanding of programming concepts
\citep{Labadze23}.

The study conducted by \cite{chan23} revealed that both undergraduate and
postgraduate students have positive attitudes towards the use of GenAI.
Furthermore, a systematic review organized by \cite{Lo24} demonstrated that
students can effectively learn from ChatGPT, resulting in improved comprehension
and academic achievement \citep{Callejo24}. Furthermore, it was observed that
ChatGPT allows students to regulate their learning pace \citep{Baha24} and can
potentially support self-regulated learning, particularly for students with
prior technical and disciplinary knowledge \citep{Xia23}.

However, researchers have also expressed concerns about the impact of these
tools on students. The systematic review by \cite{Murillo23} indicated that the
use of ChatGPT could lead to overreliance on the tool. \cite{chan23} noted that
overdependence on ChatGPT could lead to a decrease in critical thinking, as
students can make decisions based solely on the information provided by the tool.
As a potential consequence, \cite{Bastani24} observed in a study that when
programming students lost access to ChatGPT-4, those who had previously relied
on it saw their performance drop by 17\%. In contrast, students who had never
used the tool were unaffected and outperformed their peers.

The confidence of students in these tools is well founded, as shown by
\cite{Puryear22}, who found that GitHub Copilot can generate solutions for
student assignments with precision rates ranging from 68\% to 95\%. However,
\cite{Boudouaia24} observed that using ChatGPT without a structured learning
approach did not provide significant improvement over traditional self-directed
learning methods in programming. This raises concerns that students may become
overly dependent on generative AI tools, potentially hindering their learning
progress and missing opportunities to develop a deeper understanding of
fundamental concepts.

Regardless of teachers' preferences or beliefs, preliminary surveys conducted
by \cite{Dickey24} indicate that more than 54,5\% of students are already using
GenAI for homework, likely due to their perception of this technology as
advantageous and intuitive \citep{Boudouaia24}. All of these studies highlight
the need to increase the understanding of how students interact with these tools
and how they can be used to improve learning, as emphasized \cite{Lo24}.

In response to the growing need for deeper insight into the use of GenAI tools
in educational settings, this study aims to help address this gap by analyzing
the interactions between students in a Java programming course and an
educational bot powered by OpenAI gpt-3.5, enhanced with the
Retrieval-Augmented Generation (RAG) technique. Specifically, it focuses on
examining these interactions within the context of two metacognitive strategies:
spacing and interleaving. To achieve this objective, we formulate three research
questions:

\begin{itemize}
  \item RQ1 - What distribution patterns emerged in the classified student
  interaction prompts with the educational chatbot?
  \item RQ2 - Was there spacing between student prompts, and if so, which
  category led to the most rapid and consistent interactions with the bot?
  \item RQ3 - Do students' interactions with the bot alternate between study
  topics to suggest interleaving?
\end{itemize}

% What is its main limitation?

It is important to consider how GenAI tools impact students in
various demographic groups, academic disciplines, cultural backgrounds, and
levels of previous experience \citep{catalan21} \citep{neo22}. Consequently,
this study is limited to a specific group of students and focuses on the use of
a single GenAI tool. As a result, the findings may not be generalizable to other
populations or tools.

This paper is organized as follows. Section 2 presents the theoretical
framework, Section 3 describes the material and methods used in the study,
Section 4 presents the results and discussion, and Section 5 concludes the
paper and outlines future work.

%% Are there any existing solutions?
% To deal with this issue, some researchers have proposed different pedagogical
% strategies \cite{Denny24} introduced the concept of \textit{Prompt
% Problems}, where students solve programming exercises by formulating natural
% language prompts. \cite{Prasad24} proposed a self-regulated learning framework
% using GenAI to solve programming problems. \cite{Lauren23} explored the
% integration of GenAI with evidence-based learning strategies in computer science
% and engineering courses.

%% Which is the best?

% The study of effective educational strategies in use of GenAI tools in education
% context are important for determining the best training for educators. However,
% as these tools gain popularity among students \cite{Dickey24} the urgency to
% equip educators with best practices may lag behind their rapid adoption.
% Therefore, analyzing students' interaction patterns with GenAI tools is
% important for understanding how these tools are being used and how educators can
% leverage them to enhance learning. In addition, these interaction patterns can
% create new opportunities to redesign GenAI tools to better support pedagogical
% strategies without repressing the development of abstract, critical and creative
% thinking.

\section{\uppercase{Theoretical Framework}}

This section presents the theoretical framework that underpins the study. First,
we introduce the concept of metacognition used in this study. Next, we discuss
related work exploring the use of metacognitive strategies and scaffolds
in educational settings.

\begin{table*}[htbp]
  \caption{Categories - adapted from \cite{Ghimire24}}
  \begin{center}
    \renewcommand{\arraystretch}{1.6} % Increase the spacing between rows
    \begin{tabular}{p{4cm} p{12cm}} % Remove vertical bars
      \hline
      \textbf{Category} & \textbf{Description} \\
      \hline
      Debugging Help (DH) & Prompts that seek help to identify, fix errors, or understand the provided code snippet. \\
      Conceptual Question (CQ) & Prompts that are more about understanding concepts than specific code. \\
      Student Correction (SC) & Prompts where the student corrects the bot. \\
      Code Snippet  (CS) & Prompts that ask for a specific part of the code, like a function or a segment. \\
      Complete Solution (CSO) & Prompts that request an entire solution or a complete code snippet. \\
      Multiple Question (MQ) & Prompts where the user wants to solve a multiple choice exercise (Quiz). \\
      Language Change (LC) & Prompts that request a change of idiom. \\
      Uncategorized (U) & Prompts that do not fit into any of the above categories. \\
      \hline
    \end{tabular}
    \label{tab:categories}
  \end{center}
\end{table*}

\subsection{Metacognition}

Given that generative AI often produces highly accurate and automatic responses
\citep{Puryear22}, it is essential that students use these tools within an
active learning process to enhance their learning outcomes. This active
learning process can be further understood through the lens of metacognition,
which focuses on awareness of one's mental processes. \cite{flavell79}
proposed a model of metacognitive monitoring that includes four interrelated
phenomena: Metacognitive Knowledge, Metacognitive Experience, Metacognitive
Goals, and Metacognitive Actions. These processes do not occur in isolation, but
they influence each other, altering cognitive progress over time.

Metacognitive knowledge includes beliefs about variables that affect the
outcomes of cognitive activities. It is divided into three types: beliefs about
personal abilities, perceived difficulty in the task, and previously used
strategies. Metacognitive Experience refers to the feelings that arise before,
during, and after cognitive activity, such as frustration, confusion,
satisfaction, and others. Metacognitive Goals are the key to regulating thought
as they relate to the goals the individual seeks to achieve, directly
influencing the actions taken. For example, if a student’s goal is to complete
a task quickly, they may adopt a more passive approach to learning. Lastly,
Metacognitive Actions involve the planning, monitoring, and evaluation of
strategies used to achieve the goals. In terms of planning, students can
determine how to approach a task, such as spacing their study sessions
\citep{Ouhao18, Carvalho20}, interleaving topics \citep{Rivers21}, utilizing
retrieval practice \citep{larsen18}, and other possible strategies.

Spacing and interleaving are two metacognitive strategies that have been shown
to enhance learning outcomes and support the research questions of this study.
Spacing refers to the practice of distributing study sessions over time, which
has been shown to improve long-term retention and understanding of the material
\citep{Carvalho20}. Interleaving involves mixing different topics or problems
within a single learning session, which has been shown to also enhance long-term
learning and the application of student's knowledge in other contexts and
situations \citep{Rivers21}.

\subsection{Related Work}

\cite{Margulieux24} conducted a study on how undergraduate students in
introductory programming courses used generative AI to solve programming
problems in a naturalistic setting. The research focused on examining the
relationship between AI usage and students’ self-regulation strategies,
self-efficacy, and fear of failure in programming. Furthermore, the study
explored how these variables interacted with the characteristics of the learners,
the perceived usefulness of AI, and academic performance. The findings revealed
that students with higher self-efficacy, lower fear of failure or higher prior
grades tended to use AI less frequently or later in the problem solving process
and perceived it as less useful compared to their peers. However, no significant
relationship was found between students’ self-regulation strategies and their
use of AI.

\cite{Bastani24} investigated the impact of generative artificial intelligence
(AI), specifically GPT-4, on human learning, with a focus on mathematics
education in a high school. The problem addressed was how the use of generative
AI could affect the acquisition of new skills, which is crucial for long-term
productivity. The method involved a controlled experiment with approximately
one thousand students, who were exposed to two GPT-4-based tutors: a simple
tutor (GPT Base) and another with safeguards designed to promote learning
(GPT Tutor). The results showed that while access to GPT-4 improved performance
on practice exercises (48\% with GPT Base and 127\% with GPT Tutor), the removal
of access to GPT Base led to a 17\% decrease in student performance on exams.
This indicated that unrestricted use of *GPT Base* could hinder learning.
However, the GPT Tutor was able to mitigate this negative effect.

\cite{Boudouaia24} investigated the impact of ChatGPT-facilitated programming on
college students’ programming behaviors, performance, and perceptions. The
problem addressed was whether the use of ChatGPT in programming education could
influence how students interacted with programming tasks, their learning
outcomes, and their attitudes toward the technology. The study employed a
quasi-experimental design involving 82 college students divided into two
groups: one using ChatGPT-assisted programming (CFP) and the other using
traditional self-directed programming (SDP). The method combined mixed data
collection techniques, including behavioral logs, programming performance
evaluations, and interviews. The results showed that students using the CFP mode
exhibited more frequent behaviors such as debugging and reading feedback.
Although CFP students had a higher average score in programming performance,
there was no statistically significant difference compared to the SDP group
However, students’ perceptions of ChatGPT improved significantly after the
intervention, with increased perceived usefulness, ease of use, and intention to
continue using the tool in the future.

\cite{Sidra24} addressed the growing importance of metacognitive skills in an
AI-enabled workforce. The problem discussed was how AI systems, especially
collaborative tools like large language models (LLMs), impacted human
decision-making and the challenges they posed due to the lack of human-like
metacognition in AI. The method used identified four main characteristics that
differentiated human-AI interaction from human-human collaboration and proposed,
based on the dual-process theory of human thought, that metacognitive thinking
was crucial for the effective use of AI. The results indicated that human
workers needed to develop and apply metacognitive skills, such as planning,
monitoring, and evaluating their thought processes, to mitigate cognitive biases
and improve decision-making when working with AI. The study concluded by
recommending training and AI design improvements to enhance workers’
metacognitive abilities, ensuring more effective collaboration with AI systems.

Learning based on metacognitive strategies has been shown to be effective in
enhancing students’ performance and skills. \cite{Zheng19} investigated the
effects of metacognitive scaffolds on group behavior, performance, and cognitive
load in computer-supported collaborative environments. The results showed
positive impacts on metacognitive behavior and group performance without
increasing the cognitive load. \cite{LiWei23} proposed a metacognition-based
collaborative approach to enhance performance in collaborative programming. The
findings indicated that this approach influenced computational thinking,
critical thinking, and metacognitive awareness. Similarly, \cite{Wang23}
explored how metacognitive instruction affects computational thinking, critical
thinking, and metacognitive skills in collaborative programming, observing
improvements in problem-solving and collaboration. \cite{Khusnul24} investigates
the efficacy of a meta-learning approach in improving metacognitive and creative
skills. This study confirms the effectiveness of meta-learning strategies and
elucidates the relationship between meta-learning and metacognition. This
research suggests that metalearning can improve metacognitive abilities,
providing valuable insights into educational technology and course design in
higher education settings.

\section{\uppercase{Material and Method}}

This section outlines the tools and procedures used in this study. CharlieBot
served as the GenAI tool for educational purposes, and the method was structured
into three distinct phases, each designed to systematically assess its
interaction with students.

\subsection{CharlieBot}

CharlieBot is an educational bot built on ChatGPT 3.5 and enhanced with
Retrieval-Augmented Generation (RAG) technique to improve the precision of
responses provided to students \cite{Sun24}. The RAG approach was developed
using materials from a MOOC, including text, exercises, video transcripts, and
other resources.

\subsection{Method}

The study was conducted in three phases: data collection,
categorization, and analysis. During the data collection phase, students
enrolled in a second semester Java course at a public university in Spain were
introduced to CharlieBot and allowed to use it freely, without being subjected
to a specific methodology for its use. Data were collected anonymously, ensuring
that the records of interactions could not be linked to any individual student
or their academic performance.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.62]{img/figure1.png}
  \caption{Categorization of messages.}
  \label{fig:graph1}
\end{figure}

\begin{figure*}[htbp]
  \centering
  \includegraphics[scale=0.45]{img/figure2.png}
  \caption{Examples of conversations.}
  \label{fig:graph2}
\end{figure*}

In the categorization phase, the students' messages were manually classified
into eight distinct categories for analysis, representing different types of
prompt that students use when interacting with the bot. Initially,
\cite{Ghimire24} proposed four categories: Debugging Help (DH), Conceptual
Question (CQ), Code Snippet (CS), and Complete Solution (CSO). However, the data
collected indicated the need for additional categories, leading to the inclusion
of four more: Multiple Question (MQ), Student Corrections (SC), Language Change
(LC), and Uncategorized (U). In this study, students were free to use the bot as
they wanted, which contributed to the emergence of these additional categories.
Table \ref{tab:categories} presents these categories along with their respective
descriptions. To facilitate the analysis of interleaving, the prompt topics were
also categorized according to the course subjects, which are: (1) Java, (2)
Object Orientation, (3) Testing, (4) Recursion, (5) Data Structures, and (6)
Sorting and Searching Algorithms.

The analysis phase involved the use of Python scripts to extract information
from previously classified data. The datasets and code used in this process are
available at: https://github.com/rodrigoprestesmachado/uc3m.

\begin{figure*}[htbp]
  \centering
  \includegraphics[scale=0.39]{img/figure3.png}
  \caption{Average interaction time per category.}
  \label{fig:graph3}
\end{figure*}

\section{\uppercase{Results and Discussion}}

A total of 625 student messages were categorized in 81 conversations, with
an average of 7.72 messages per conversation. This data offers insights into how
users engage with CharlieBot and the types of prompts they submit. The results
of the analysis are presented in this section, addressing the research questions
outlined in the introduction. Figure \ref{fig:graph1} shows the distribution of
the messages in the categories.

\subsection{Categorization of messages}

About the first research question: \textit{RQ1: What distribution patterns
emerged in the classified student interaction prompts with the educational
chatbot?}

According to Figure \ref{fig:graph1} messages classified as
Conceptual Question account for 35.8\% of the total. Sequences of Conceptual
Questions often start from a practical perspective of a given code, as
illustrated in conversation 4 of Figure \ref{fig:graph2}, which presents
examples of students' interactions with the bot. In other cases, the
conversation consists entirely of messages classified as Conceptual Question,
such as in conversation 15 of Figure \ref{fig:graph2}.

As shown in Figure \ref{fig:graph1}, approximately 29.9\% of the messages were
classified as Debugging Help. Debugging help messages are likely to provide
students with a practical approach to understanding code, identifying errors,
and building skills to enhance their debugging abilities for future tasks.

Most of the messages are located in categories Conceptual Question and Debugging
Help corroborate the findings of \cite{Ghimire24} which showed that the
questions are also localized in the same categories.

Multiple Question exercise resolutions represented 14.9\% of the responses.
These prompts typically involved students submitting one or more exercises to
CharlieBot and requesting solutions. The conversation 13 in \ref{fig:graph2}
illustrates a sequence of multiple choice questions.

Requests for the bot to generate a Code Snippet or Complete Solution accounted
for 8.2\% of student messages. These prompts reflected a desire for more
direct answers; however, it was observed that, after receiving these solutions,
many students transitioned to a more active approach in seeking to understand
the code or the underlying concepts. Thus, it can be inferred that the prompts
within the Code Snippet or Complete Solution category were often used as a
starting point for a more in-depth study.

Student corrections to bot responses accounted for 4.2\% of the interactions.
In most cases, these corrections occurred when the student already had the
correct answer to an exercise and identified an error in the bot response, as
illustrated in conversation 2 in Figure \ref{fig:graph2}. These corrections
highlight that, although the bot provides quick feedback, it is not always
accurate.

Figure \ref{fig:graph1} also indicates that approximately 5.8\% of the prompts
written by students were classified as Uncategorized. These messages include
expressions of gratitude toward the bot, such as a simple '\textit{thanks}',
contextual statements like '\textit{I'm reviewing object orientation}', as well
as requests for additional exercises and summaries. Depending on the type of
investigation, there is opportunity for these messages to be further subdivided
into new categories.

Finally, 1.3\% of the messages were classified as Language Change. These prompts
were typically requests for the bot to switch languages, as seen in conversation
9 in Figure \ref{fig:graph2}.

\begin{table*}[htbp]
  \caption{Average and Standard Deviation of Different Categories (in minutes)}
  \begin{center}
    \renewcommand{\arraystretch}{1.6} % Increase the spacing between rows
    \begin{tabular}{p{4cm} p{4cm} p{4cm}} % Remove vertical bars
      \hline
      \textbf{Category} & \textbf{Average} & \textbf{Standard Deviation} \\
      \hline
      Code Snippet & 1.26 & 1.74 \\
      Uncategorized & 1.65 & 3.54 \\
      Language Change & 1.89 & 3.28 \\
      Multiple Question & 2.65 & 6.10 \\
      Student Correction & 3.17 & 7.33 \\
      \hdashline
      \textit{Overall} & \textit{3.43} & \textit{6.60} \\
      \hdashline
      Debugging Help & 4.84 & 10.13 \\
      Conceptual Questions & 5.41 & 10.54 \\
      Complete Solution & 6.55 & 10.14 \\
      \hline
    \end{tabular}
    \label{tab:averages}
  \end{center}
\end{table*}

\subsection{Spacing}

About the second research question: \textit{RQ2: Was there spacing between
student prompts, and if so, which category led to the most rapid and consistent
interactions with the bot?}

For the analysis of the spacing, we initially considered a minimum interval of
60 minutes as a reference to define the significant spacing \citep{Gadella24}.
Based on this, our results showed that, of the 81 conversations analyzed, only
27 (33.3\%) had at least one message with an interval greater than 60 minutes.
Figure 2 illustrates the elapsed time between messages (delta). For example,
in conversation 15, the first message was sent at time zero, while the second
message was sent by the student after a delta of 0.6 minutes. Additionally,
message 4 in conversation 15, as shown in Figure \ref{fig:graph2}, presents a
delta of 69.1 minutes, which, according to our reference, characterizes
significant spacing. Figure \ref{fig:graph3} also shows the number of spacings
in each conversation. Among conversations that exhibited spacing, the
average number of subsections per conversation was 2.2. Therefore, a
conversation with one instance of spacing is typically divided into two distinct
study subsections.

This data suggests that there is indeed little spacing between study sessions.
It is worth noting that, in order to create a new section in CharlieBot,
students must close the browser tab and start a new conversation, which results
in losing the previous chat history.

\subsubsection{Interaction Time per Category}

When a student requests a code snippet (CS), it is generally predictable that
they will resume interaction with the bot within a short period of time, as
indicated by the mean (1.26) and standard deviation (1.74) of the Code Snippet
category in Table \ref{tab:averages}. This result is noteworthy because,
although some requests are simple and lead to quick responses, others may be
more complex and would require more time for reflection. However, the low
average response time and minimal variability may indicate that students adopt a
more passive learning posture when requesting the bot to generate small pieces
of code.

Despite some occasional variation (3.54), prompts classified as Uncategorized
exhibit a relatively low average time (1.65) for students to resume interaction
with the bot. This variation occurs, in part, due to situations where students
end the conversation with an uncategorized message, such as a simple
\textit{'thanks'} but later choose to resume the interaction.

When a student requests the bot to change the language (LC) in a response, the
new interaction happens quickly (1.89) with little variation in response time
(3.28), meaning it is predictable that the student will continue interacting.

Although still below the overall average, students take a moderate amount of
time (2.65) to resume interaction with the bot after requesting the solution to
a multiple-choice question (MQ). The variation (6.10) suggests that some
responses may lead to a slower follow-up interaction, possibly indicating a
moment of deeper reflection on the part of the students. However, since both the
mean and standard deviation are below the overall average, this could indicate
a more passive behavior, as students obtain ready-made answers from the bot.

% This less active learning approach can occur for various reasons, ranging from
% the belief that reviewing solutions to similar questions enhances learning, to
% issues such as negligence, poor planning, or other possible factors.

The average time for students to resume interaction in the correction category
(SC) is close to the overall average (3.17), although the high standard
deviation (7.33) indicates significant variation. In some cases, students are
already familiar with the answer and only seek a quick correction from the bot;
in others, they detect flaws in the bot's responses and need to carefully
examine the subsequent answer to identify potential errors, which suggests a
more active behavior.

After a question classified as Debugging Help (DH), there is both a
longer-than-average response time (4.84) and a high variation in the time taken
for a new interaction (10.13). This suggests that student prompts in this
category tend to be more time-consuming and unpredictable. A possible
explanation is that some DH issues are simple, such as '\textit{what does x++ do?}',
while others involve requests for analysis of more complex code, like
\textit{'explain this piece of code (accompanied by a complete method).'}

Conceptual questions (CQ) generally result in a high average time (5.41) for
students to re-engage with the bot. Moreover, the high standard deviation
(10.54) indicates an unpredictable variation in response times. This
unpredictability may be attributed to the fact that, in some cases, the
complexity of the question and answer demands more reflection time from the
student, whereas in others, lesser complexity allows for a quicker interaction
with the bot.

Finally, a message classified as a Complete Solution request (SCO) exhibits the
longest average time (6.55) for students to resume interaction with the bot.
Additionally, the high standard deviation (10.14) highlights significant
unpredictability in response time. After requesting a complete solution,
students tend to take longer to return, due to the complexity of the problems
involved, such as the implementation of sorting algorithms like Heapsort. Table
\ref{tab:averages} presents the means and standard deviations for each
category.

However, some categories suggest quicker interactions with less reflection on
the part of students, such as 'Code Snippet' and 'Multiple Choice.' On the other
hand, other categories indicate that students may be in a more reflective
process, such as 'Debugging Help,' 'Conceptual Question,' and 'Complete
Solution.' This demonstrates that the level of student engagement varies
according to the nature of the requested interaction, reflecting different
approaches and depths in the learning process.

\subsection{Interleaving}

About the third research question: \textit{RQ3: Do students' interactions with
the bot alternate between study topics to suggest interleaving?}

To analyze the interweaving of topics, sections were initially divided into
those that exhibited spacing and those that did not.

In conversations with the bot that did not include spacing (66.7\% of the total
conversations), approximately 19.2\% of the messages resulted in a change of
topic. Qualitative analysis of what led to this interweaving indicates that
part of it was due to students' requests for assistance in solving exercises
from different topics, suggesting a less active learning approach, as the time
interval for this type of interaction tends to be shorter than average.
Additionally, some instances of interweaving were observed after a prolonged
period of 20 to 50 minutes, still below the 60-minute threshold established as
the cutoff for considering spacing.

In interactions with the bot that included spacing (33.3\% of the total
conversations), approximately 31.4\% of the messages switched topics.
Qualitative analysis of these interactions reveals that a significant portion
of this interweaving was caused by spacing. In other words, some students use
the bot for quick consultations, asking a question and then returning hours
later with another question on a different subject.

Thus, the data revealed that the existing interweaving is limited and does not
suggest an effective study strategy, as it mainly arises from requests for
exercise resolution and from the natural spacing pattern of a consultation
interaction.

\section{Conclusion and Future Work}

The objective of this work was to analyze the interaction of second-semester
Java programming students with an educational bot developed using the RAG
technique, employing two metacognitive strategies: spacing and interleaving.

Initially, the theory of metacognition, alongside the classification of student
actions, has proven effective in understanding and revealing how students
utilize the bot. Although the data shows that most messages fall within the
categories of Conceptual Questions and Debugging Help categories identified as
promoting slower but potentially deeper engagement with the bot, indicative of a
more active form of learning—the findings also reveal that students' use of
spacing and interleaving strategies remains quite limited. Encouraging students
to apply these strategies more intentionally could significantly enhance their
learning outcomes. To further explore this, a broader study is planned to
examine, for instance, the relationship between student profiles and their
strategies in using a Generative AI tool. Additionally, to support educators
in analyzing student behavior and providing more targeted assistance, an AI
model is being developed to automatically classify student messages, offering
valuable insights into the ways these tools are being employed.

% The results suggest that most student interactions with GenAI tools are centered
% on active learning strategies, although the deliberate use of interleaving and
% spacing techniques remains limited. Guiding students toward a more intentional
% application of these strategies could enhance their learning outcomes. The
% anonymity of interactions leaves it unclear whether these strategies were used
% deliberately or emerged naturally, highlighting the need for future research on
% students' conscious application of these methods. To support educators in
% analyzing student behavior and offering more targeted assistance, an AI model
% is being developed to automatically classify student messages, providing
% valuable insights into how these tools are being utilized.


\bibliographystyle{apalike}
{\small
\bibliography{References}}

\end{document}


% % Conceptual Question
% \subsubsection*{Sequence of Conceptual Question}

% Messages classified as conceptual question represent around 32\% of all
% messages. We have found a pattern of sequences of Conceptual Question in some
% conversations. Some conversations are entirely composed of messages classified
% as Conceptual Question. In other cases, the sequence of Conceptual question
% starts with a request for a Complete Solution or a Code Snippet, followed by
% a progression of Conceptual Question. In both cases, the student is actively
% engaging with the bot to understand the concepts and compromised with more
% deep learning.

% Regarding Metacognitive Knowledge, these sequences of prompts classified as
% Conceptual Question, can be interpreted that the students are aware they have
% not fully mastered a concept and are trying to understand it, demonstrating a
% good level of self-awareness. In this sense, the student realizes that the
% problem requires an understanding of concepts in order to advance in their
% studies. In this way, the student applies an active learning strategy.

% About Metacognitive Experience, several types of feelings may be associated at
% this moment, such as actively reflecting on the answers, uncertainty and/or
% confusion when trying to understand the concept, frustration for not
% progressing, and satisfaction as their questions are answered in a way that
% facilitates understanding.

% Regarding the type of Metacognition related to objective of the Task, the
% students may recognize the difficult and trying to achieve a deeper
% understanding of the evolved concepts to later apply in practice.

% With respect to Metacognitive Strategies, the student demonstrates a sence of
% planning to identify their knowledge gaps. They may start with more general
% questions to obtain an overview of the concept and then ask more specific
% questions as they gain more clarity. Because the questions are in sequence, the
% student probably can adequately monitor their progress.

% % Multiple Choice
% \subsubsection*{Sequence of Multiple Choice}

% Around 16.9\% of the messages were classified as Multiple Choice. These prompts
% typically involve students sending one or more exercises to CharlieBot, asking
% for the solution.

% We identified a sequence of Multiple Choice patterns in some of the
% conversations. From the point of view of Metacognitive Knowledge, a possible
% interpretation of this complex phenomenon is that students may believe in their
% inability to answer several questions. They may also assume that the questions
% share a common theme, and solving them together may help consolidate knowledge.
% Additionally, they might understand that obtaining the answers quickly is more
% efficient.

% Regarding Metacognitive Experiences, students may experience feelings such as
% cognitive overload, a sense of urgency, and/or confidence in ready-made answers,
% which can often lead to relief and a reduction of frustration.

% Concerning the Metacognitive about the Task, one interpretation may be that the
% student simply wants to complete the task quickly to alleviate the workload,
% and/or obtain a correct answer without focusing on deeper learning at
% this moment.

% In relation to Metacognitive Strategies, students are likely focusing on saving
% time and/or possibly lacking planning skills. They may also perceive that
% solving the questions on their own requires too much cognitive effort, but this
% pattern might reveal a lack of progress monitoring and understanding of
% concepts. After obtaining the answers, the metacognitive evaluation process
% could lead to feelings of satisfaction, but further reflection may indicate that
% the strategy was not effective in supporting deep learning. Depending on this
% evaluation, it may influence future decision-making to change the strategy.

% % Debugging help
% \subsubsection*{Sequence of Debugging Help}

% Around 30.9\% of the messages were classified as Debugging Help. Besides of
% that, we identified a pattern of a sequences of Debugging Help in the
% conversations. These prompts typically start with a message requesting a
% Complete Solution or a Code Snippet. After that, the students normally chance
% from passive to more active learning strategy.

% In terms of Metacognitive Knowledge, the student probably recognizes that they
% do not have the necessary tools or knowledge to debug the code on their own.
% Concerning the understanding of the task, they may know that debugging is not
% just a matter of superficial corrections but requires understanding the
% underlying logic. Thus, the student apparently believes that the sequence of
% questions related to debugging will help them understand the codes.

% In terms of Metacognitive Experiences, some feelings may arise at this moment,
% such as frustration or uncertainty in the face of errors, confusion during the
% process, and also growing confidence or satisfaction as they better understand
% the code and possible corrections.

% With respect to Metacognitive related with the objective of the Task, the
% student is probably focused on understanding the code in practical terms,
% identifying errors, and learning how to better debug in the future.

% As for Metacognitive associated with the Strategy, the student has requested
% external help and realizes that dividing the debugging task into several
% questions can result in a better understanding of the code. They can monitor
% their progress throughout the process, adjusting their questions as necessary,
% evaluate the effectiveness of the responses received, and reflect on what was
% learned during the process.
