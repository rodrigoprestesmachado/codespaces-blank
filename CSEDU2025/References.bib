@inproceedings{Ghimire24,
  author    = {Ghimire, Aashish
               and Edwards, John},
  editor    = {Olney, Andrew M.
               and Chounta, Irene-Angelica
               and Liu, Zitao
               and Santos, Olga C.
               and Bittencourt, Ig Ibert},
  title     = {Coding with AI: How Are Tools Like ChatGPT Being Used by Students in Foundational Programming Courses},
  booktitle = {Artificial Intelligence in Education},
  year      = {2024},
  publisher = {Springer Nature Switzerland},
  address   = {Cham},
  pages     = {259--267},
  abstract  = {Tools based on generative artificial intelligence (AI), such as ChatGPT, have quickly become commonplace in education, particularly in tasks like programming. We report on a study exploring how students use a tool similar to ChatGPT, powered by GPT-4, while working on Introductory Computer Programming (CS1) assignments, addressing a gap in empirical research on AI tools in education. Utilizing participants from two CS1 class sections, our research employed a custom GPT-4 tool for assignment assistance and the ShowYourWork plugin for keystroke logging. Prompts, AI replies, and keystrokes during assignment completion were analyzed to understand the state of students' programs when they prompt the AI, the types of prompts they create, and whether and how students incorporate the AI responses into their code. The results indicate distinct usage patterns of ChatGPT among students, including the finding that students ask the AI for help on debugging and conceptual questions more often than they ask the AI to write code snippets or complete solutions for them. We hypothesized that students ask conceptual questions near the beginning and debugging help near the end of program development do not find statistical evidence to support it. We find that large numbers of AI responses are immediately followed by the student copying and pasting the response into their code. The study also showed that tools like these are widely accepted and appreciated by students and deemed useful according to a student survey - suggesting that the integration of AI tools can enhance learning outcomes and positively impact student engagement and interest in programming assignments.},
  isbn      = {978-3-031-64299-9}
}

@article{Lo24,
  title    = {The influence of ChatGPT on student engagement: A systematic review and future research agenda},
  journal  = {Computers and Education},
  volume   = {219},
  pages    = {105100},
  year     = {2024},
  issn     = {0360-1315},
  doi      = {https://doi.org/10.1016/j.compedu.2024.105100},
  url      = {https://www.sciencedirect.com/science/article/pii/S0360131524001143},
  author   = {Chung Kwan Lo and Khe Foon Hew and Morris Siu-yung Jong},
  keywords = {ChatGPT, OpenAI, Student engagement, Systematic review, Artificial intelligence},
  abstract = {ChatGPT, a state-of-the-art artificial intelligence (AI) chatbot, has gained considerable attention as a transformative yet controversial tool for enhancing teaching and learning experiences. Several reviews and numerous articles have been written about harnessing ChatGPT in education since its release on November 30, 2022. Besides summarising its strengths, weaknesses, opportunities, and threats (SWOT) as identified in previous systematic reviews of ChatGPT research, this systematic review aims to develop a new understanding of its influence on student engagement by synthesising the existing related research using a three-dimensional framework comprising behavioural, emotional, and cognitive aspects. We searched relevant databases and included 72 empirical studies published within one year of ChatGPT's initial release. The findings reveal robust but narrowly focused evidence related to behavioural engagement (i.e., work with ChatGPT) and disengagement (i.e., academic dishonesty). The evidence related to the emotional aspect is mixed, with instances of both engagement (e.g., satisfaction and interest/fun) and disengagement (e.g., disappointment and worry/anxiety). There is broad but weak evidence regarding cognitive engagement (e.g., increased understanding and positive self-perception) and disengagement (e.g., reduced critical thinking and overreliance). Our review uncovers several under-explored indicators of student engagement, pointing to the need for further research. Specifically, future studies could focus on students' study habits and attendance (behavioural engagement), social interaction (emotional engagement), and self-regulation and critical thinking (cognitive engagement) in ChatGPT-supported learning environments.}
}

@article{Murillo23,
  title     = {Challenges and Opportunities of AI-Assisted Learning: A Systematic Literature Review on the Impact of ChatGPT Usage in Higher Education},
  author    = {Alfonso Renato Vargas-Murillo and Ilda Nadia Monica de la Asuncion Pari-Bedoya and Francisco de Jes\'us Guevara-Soto},
  journal   = {International Journal of Learning, Teaching and Educational Research},
  volume    = {22},
  number    = {7},
  pages     = {122-135},
  year      = {2023},
  publisher = {International Journal of Learning, Teaching and Educational Research},
  doi       = {10.26803/ijlter.22.7.7},
  url       = {http://ijlter.org/index.php/ijlter}
}

@article{cai23,
  title     = {Factors influencing learner attitudes towards ChatGPT-assisted language learning in higher education},
  author    = {Cai, Qianqian and Lin, Yupeng and Yu, Zhonggen},
  journal   = {International Journal of Human--Computer Interaction},
  pages     = {1--15},
  year      = {2023},
  publisher = {Taylor \& Francis}
}

@article{chan23,
  title     = {Students’ voices on generative AI: Perceptions, benefits, and challenges in higher education},
  author    = {Chan, Cecilia Ka Yuk and Hu, Wenjie},
  journal   = {International Journal of Educational Technology in Higher Education},
  volume    = {20},
  number    = {1},
  pages     = {43},
  year      = {2023},
  publisher = {Springer}
}

@article{zhang24,
  title     = {A systematic review of ChatGPT use in K-12 education},
  author    = {Zhang, Peng and Tur, Gemma},
  journal   = {European Journal of Education},
  volume    = {59},
  number    = {2},
  pages     = {e12599},
  year      = {2024},
  publisher = {Wiley Online Library}
}

@article{wu24,
  title     = {Promoting self-regulation progress and knowledge construction in blended learning via ChatGPT-based learning aid},
  author    = {Wu, Ting-Ting and Lee, Hsin-Yu and Li, Pin-Hui and Huang, Chia-Nan and Huang, Yueh-Min},
  journal   = {Journal of Educational Computing Research},
  volume    = {61},
  number    = {8},
  pages     = {3--31},
  year      = {2024},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA}
}

@article{Puryear22,
  author     = {Puryear, Ben and Sprint, Gina},
  title      = {Github copilot in the classroom: learning to code with AI assistance},
  year       = {2022},
  issue_date = {November 2022},
  publisher  = {Consortium for Computing Sciences in Colleges},
  address    = {Evansville, IN, USA},
  volume     = {38},
  number     = {1},
  issn       = {1937-4771},
  abstract   = {Recent advances in deep machine learning have enabled artificial intelligence-driven development environments (AIDEs). AIDEs are programming tools that, given comments or starter code, can generate code solution suggestions. As the accuracy of these tools continues to increase, one particular AIDE from Github, Copilot, has been gaining significant attention for its performance and ease of use. The rise of Copilot suggests that code solution generation tools will soon be commonplace in both the industry and in computer science courses, with expert and novice programmers alike benefiting from using these tools. More specifically for novices, the effects of Copilot on the process of learning to code are mostly unknown. In this paper, we perform initial explorations into these effects. Using introductory computer science and data science courses, we evaluate Copilot-generated programming assignment solutions for correctness, style, skill level appropriateness, grade scores, and potential plagiarism. Our findings indicate Copilot generates mostly unique code that can solve introductory assignments with human-graded scores ranging from 68\% to 95\%. Based on these results, we provide recommendations for educators to help adapt their courses to incorporate new AIDE-based programming workflows.},
  journal    = {J. Comput. Sci. Coll.},
  month      = {nov},
  pages      = {37–47},
  numpages   = {11}
}

@inproceedings{Denny24,
  author    = {Denny, Paul and Leinonen, Juho and Prather, James and Luxton-Reilly, Andrew and Amarouche, Thezyrie and Becker, Brett A. and Reeves, Brent N.},
  title     = {Prompt Problems: A New Programming Exercise for the Generative AI Era},
  year      = {2024},
  isbn      = {9798400704239},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3626252.3630909},
  doi       = {10.1145/3626252.3630909},
  abstract  = {Large language models (LLMs) are revolutionizing the field of computing education with their powerful code-generating capabilities. Traditional pedagogical practices have focused on code writing tasks, but there is now a shift in importance towards reading, comprehending and evaluating LLM-generated code. Alongside this shift, an important new skill is emerging -- the ability to solve programming tasks by constructing good prompts for code-generating models. In this work we introduce a new type of programming exercise to hone this nascent skill: 'Prompt Problems'. Prompt Problems are designed to help students learn how to write effective prompts for AI code generators. A student solves a Prompt Problem by crafting a natural language prompt which, when provided as input to an LLM, outputs code that successfully solves a specified programming task. We also present a new web-based tool called Promptly which hosts a repository of Prompt Problems and supports the automated evaluation of prompt-generated code. We deploy Promptly in one CS1 and one CS2 course and describe our experiences, which include student perceptions of this new type of activity and their interactions with the tool. We find that students are enthusiastic about Prompt Problems, and appreciate how the problems engage their computational thinking skills and expose them to new programming constructs. We discuss ideas for the future development of new variations of Prompt Problems, and the need to carefully study their integration into classroom practice.},
  booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
  pages     = {296–302},
  numpages  = {7},
  keywords  = {ai code generation, artificial intelligence, generative ai, large language models, llms, prompt engineering, prompt problems},
  location  = {Portland, OR, USA},
  series    = {SIGCSE 2024}
}

@inproceedings{Prasad24,
  author    = {Prasad, Prajish and Sane, Aamod},
  title     = {A Self-Regulated Learning Framework using Generative AI and its Application in CS Educational Intervention Design},
  year      = {2024},
  isbn      = {9798400704239},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3626252.3630828},
  doi       = {10.1145/3626252.3630828},
  abstract  = {Self-regulation refers to the ability to plan, monitor, control and reflect on one's problem-solving process. Prior research has shown that self-regulated learning (SRL) strategies help improve novice performance in solving programming problems. However, with the advent of LLM tools like ChatGPT, novices can generate fairly accurate code by just providing the problem prompt, and hence may forego applying essential self-regulation strategies such as planning and reflection to solve the problem. In this position paper, we discuss challenges and opportunities that generative AI technologies pose for novices' self-regulation strategies in the context of programming problem solving. We believe that the key challenge facing educators is that such technologies may hamper novices' ability to regulate their programming problem solving process.On the other hand, these technologies also open up the possibility to design new interventions that promote better SRL strategies in learners. We draw on generic and domain-specific self-regulated learning theories as the basis of our work, and propose an SRL framework that incorporates use of generative AI tools in programming problem solving. We illustrate how the proposed framework guides exploration of the design space of interventions that integrate generative AI in CS education.},
  booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
  pages     = {1070–1076},
  numpages  = {7},
  keywords  = {chatgpt, generative ai, llm, metacognition, pair programming, pair thinking, self-regulated learning, self-regulation, srl},
  location  = {Portland, OR, USA},
  series    = {SIGCSE 2024}
}

@inproceedings{Lauren23,
  author    = {P. Lauren and P. Watta},
  booktitle = {2023 IEEE Frontiers in Education Conference (FIE)},
  title     = {Work-in-Progress: Integrating Generative AI with Evidence-based Learning Strategies in Computer Science and Engineering Education},
  year      = {2023},
  volume    = {},
  issn      = {},
  pages     = {1-5},
  abstract  = {Generative AI assistants are AI-powered applications that can provide personalized responses to user queries or prompts. A variety of AI assistants have recently been released, and among the most popular is OpenAI&#x27;s ChatGPT. In this work-in-progress in innovative practice, we explore evidence-based learning strategies and the integration of Generative AI for computer science and engineering education. We expect this research will lead to innovative pedagogical approaches to enhance undergraduate computer science and engineering education. In particular, we describe how ChatGPT was used in two computing-based courses: a Junior-level course in database systems and a Senior-level class in mobile application development. We identify four evidence-based learning strategies: well-defined learning goals, authentic learning experiences, structured learning progression, and strategic assessment. We align these strategies with the two aforementioned courses and evaluate the usefulness of ChatGPT specifically in achieving the learning goals. Combining Generative AI with evidence-based learning has the potential to transform modern education into a more personalized learning experience.},
  keywords  = {computer science;transforms;chatbots;mobile communication;database systems;artificial intelligence;engineering education},
  doi       = {10.1109/FIE58773.2023.10342970},
  url       = {https://doi.ieeecomputersociety.org/10.1109/FIE58773.2023.10342970},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {oct}
}


@article{Dickey24,
  author     = {Dickey, Ethan and Bejarano, Andres and Garg, Chirayu},
  title      = {AI-Lab: A Framework for Introducing Generative Artificial Intelligence Tools in Computer Programming Courses},
  year       = {2024},
  issue_date = {Jul 2024},
  publisher  = {Springer-Verlag},
  address    = {Berlin, Heidelberg},
  volume     = {5},
  number     = {6},
  url        = {https://doi.org/10.1007/s42979-024-03074-y},
  doi        = {10.1007/s42979-024-03074-y},
  journal    = {SN Comput. Sci.},
  month      = {jul},
  numpages   = {17},
  keywords   = {Generative AI (GenAI), Core skill development, Junior-Year Wall, Pedagogical framework, AI-Lab, ChatGPT}
}


@article{Baha24,
  author     = {Ait Baha, Tarek and El Hajji, Mohamed and Es-Saady, Youssef and Fadili, Hammou},
  title      = {The impact of educational chatbot on student learning experience},
  year       = {2023},
  issue_date = {Jun 2024},
  publisher  = {Kluwer Academic Publishers},
  address    = {USA},
  volume     = {29},
  number     = {8},
  issn       = {1360-2357},
  url        = {https://doi.org/10.1007/s10639-023-12166-w},
  doi        = {10.1007/s10639-023-12166-w},
  abstract   = {Artificial Intelligence (AI) technologies have increasingly become vital in our everyday lives. Education is one of the most visible domains in which these technologies are being used. Conversational Agents (CAs) are among the most prominent AI systems for assisting teaching and learning processes. Their integration into an e-learning system can provide replies suited to each learner's specific needs, allowing them to study at their own pace. In this paper, based on recent advancements in Natural Language Processing (NLP) and deep learning techniques, we present an experimental implementation of an educational chatbot intended to instruct secondary school learners Logo, an educational programming language. The related chatbot was implemented and evaluated in Moroccan public schools with the support of teachers from the Regional Center for Education and Training Professions of Souss Massa. The experiments included 109 students grouped into three separate classes. One is a control class group that uses a traditional approach, while the other two are experimental groups that employ digital content and the chatbot-based method. Preliminary findings indicate that employing chatbots can greatly enhance student learning experiences by allowing them to study at their own speed with less stress, saving them time, and keeping them motivated. Furthermore, integrating these AI systems into a smart classroom will not only create a supportive environment by encouraging good interactions with students, it will also allow learners to be more engaged and achieve better academic objectives.},
  journal    = {Education and Information Technologies},
  month      = {sep},
  pages      = {10153–10176},
  numpages   = {24},
  keywords   = {Educational chatbot, Learning experience, Virtual tutor, Conversational agent, E-learning, Chatbot's usability}
}

@article{catalan21,
  title   = {Conversational agent for supporting learners on a MOOC on programming with Java},
  author  = {Catal{\'a}n, Aguirre Cristina and Gonz{\'a}lez-Castro, Nuria and Delgado, Kloos Carlos and Alario-Hoyos, Carlos and Mu{\~n}oz-Merino, Pedro J},
  journal = {Computer Science and Information Systems},
  volume  = {18},
  number  = {4},
  pages   = {1271--1286},
  year    = {2021}
}

@article{neo22,
  title     = {THE MERLIN PROJECT: MALAYSIAN STUDENTS’ACCEPTANCE OF AN AI CHATBOT IN THEIR LEARNING PROCESS},
  author    = {Neo, Mai},
  journal   = {Turkish Online Journal of Distance Education},
  volume    = {23},
  number    = {3},
  pages     = {31--48},
  year      = {2022},
  publisher = {Anadolu University}
}

@article{Sun24,
  title   = {Benchmarking Large Language Models in Retrieval-Augmented
             Generation},
  volume  = {38},
  url     = {https://ojs.aaai.org/index.php/AAAI/article/view/29728},
  doi     = {10.1609/aaai.v38i16.29728},
  number  = {16},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author  = {Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le},
  year    = {2024},
  month   = {Mar.},
  pages   = {17754-17762}
}


@article{Yilmaz23,
  title    = {Augmented intelligence in programming learning: Examining student
              views on the use of ChatGPT for programming learning},
  journal  = {Computers in Human Behavior: Artificial Humans},
  volume   = {1},
  number   = {2},
  pages    = {100005},
  year     = {2023},
  issn     = {2949-8821},
  doi      = {https://doi.org/10.1016/j.chbah.2023.100005},
  url      = {https://www.sciencedirect.com/science/article/pii/S2949882123000051},
  author   = {Ramazan Yilmaz and Fatma Gizem {Karaoglan Yilmaz}},
  keywords = {Generative artificial intelligence, ChatGPT, Programming,
              Programming learning, Student opinions}
}

@article{flavell79,
  title     = {Metacognition and cognitive monitoring: A new area of cognitive--developmental inquiry.},
  author    = {Flavell, John H},
  journal   = {American psychologist},
  volume    = {34},
  number    = {10},
  pages     = {906},
  year      = {1979},
  publisher = {American Psychological Association}
}


@article{Carvalho20,
  author   = {Carvalho, Paulo F.
              and Sana, Faria
              and Yan, Veronica X.},
  title    = {Self-regulated spacing in a massive open online course is related to better learning},
  journal  = {npj Science of Learning},
  year     = {2020},
  month    = {Mar},
  day      = {16},
  volume   = {5},
  number   = {1},
  pages    = {2},
  abstract = {In this study, we examined students' natural studying behaviors in massive, open, online course (MOOC) on introductory psychology. We found that, overall, distributing study across multiple sessions---increasing spacing---was related to increased performance on end-of-unit quizzes, even when comparing the same student across different time-points in the course. Moreover, we found important variation on who is more likely to engage in spaced study and benefit from it. Students with higher ability and students who were more likely to complete course activities were more likely to space their study. Spacing benefits, however, were largest for the lower-ability students and for those students who were less likely to complete activities. These results suggest that spaced study might work as a buffer, improving performance for low ability students and those who do not engage in active practices. This study highlights the positive impact of spacing in real-world learning situations, but more importantly, the role of self-regulated learning decisions in shaping the impact of spaced practice.},
  issn     = {2056-7936},
  doi      = {10.1038/s41539-020-0061-1},
  url      = {https://doi.org/10.1038/s41539-020-0061-1}
}

@article{Rivers21,
  author   = {Firth, Jonathan and Rivers, Ian and Boyle, James},
  title    = {A systematic review of interleaving as a concept learning strategy},
  journal  = {Review of Education},
  volume   = {9},
  number   = {2},
  pages    = {642-684},
  keywords = {interleaving, concept learning, memory, transfer, education},
  doi      = {https://doi.org/10.1002/rev3.3266},
  url      = {https://bera-journals.onlinelibrary.wiley.com/doi/abs/10.1002/rev3.3266},
  eprint   = {https://bera-journals.onlinelibrary.wiley.com/doi/pdf/10.1002/rev3.3266},
  abstract = {Abstract A systematic review was conducted into the effect of interleaving the order of examples of concepts in terms of both memory of items and transfer to new items. This concept has important implications for how and when teachers present examples in the classroom. A total of 26 studies met the inclusion criteria; a subset of 17 studies (with 32 constituent datasets) formed the basis of a meta-analysis, and the remainder were analysed within a narrative review. Memory (as tested by presenting studied items from a learned category) showed an interleaving benefit with effect sizes (Hedges’ g) of up to 0.65, and transfer (as tested by presenting novel items from a learned category) a benefit with effect sizes of up to 0.66. Interleaving was found to be of greatest use when differences between items are subtle, and the benefit extended to both art- and science-based items, with implication for practitioner decisions over how and when to apply the technique. It also extended to delayed tests. The review revealed that the literature is dominated by laboratory studies of university undergraduates, and the need for future school-based research using authentic classroom tasks is outlined.},
  year     = {2021}
}


@article{Ouhao18,
  author     = {Chen, Ouhao and Castro-Alonso, Juan C. and Paas, Fred and Sweller, John},
  date       = {2018/06/01},
  doi        = {10.1007/s10648-017-9426-2},
  id         = {Chen2018},
  isbn       = {1573-336X},
  journal    = {Educational Psychology Review},
  number     = {2},
  pages      = {483--501},
  title      = {Extending Cognitive Load Theory to Incorporate Working Memory Resource Depletion: Evidence from the Spacing Effect},
  url        = {https://doi.org/10.1007/s10648-017-9426-2},
  volume     = {30},
  year       = {2018},
  bdsk-url-1 = {https://doi.org/10.1007/s10648-017-9426-2}
}

@inproceedings{larsen18,
  title        = {Planning education for long-term retention: the cognitive science and implementation of retrieval practice},
  author       = {Larsen, Douglas P},
  booktitle    = {Seminars in neurology},
  volume       = {38},
  number       = {04},
  pages        = {449--456},
  year         = {2018},
  organization = {Thieme Medical Publishers}
}

@article{Xia23,
  author   = {Xia, Qi and Chiu, Thomas K. F. and Chai, Ching Sing and Xie, Kui},
  title    = {The mediating effects of needs satisfaction on the relationships between prior knowledge and self-regulated learning through artificial intelligence chatbot},
  journal  = {British Journal of Educational Technology},
  volume   = {54},
  number   = {4},
  pages    = {967-986},
  keywords = {artificial intelligence, K-12 education, prior knowledge, self-determination theory, self-regulated learning},
  doi      = {https://doi.org/10.1111/bjet.13305},
  url      = {https://bera-journals.onlinelibrary.wiley.com/doi/abs/10.1111/bjet.13305},
  eprint   = {https://bera-journals.onlinelibrary.wiley.com/doi/pdf/10.1111/bjet.13305},
  abstract = {Abstract The anthropomorphic characteristics of artificial intelligence (AI) can provide a positive environment for self-regulated learning (SRL). The factors affecting adolescents' SRL through AI technologies remain unclear. Limited AI and disciplinary knowledge may affect the students' motivations, as explained by self-determination theory (SDT). In this study, we examine the mediating effects of needs satisfaction in SDT on the relationship between students' previous technical (AI) and disciplinary (English) knowledge and SRL, using an AI conversational chatbot. Data were collected from 323 9th Grade students through a questionnaire and a test. The students completed an AI basic unit and then learned English with a conversational chatbot for 5 days. Confidence intervals were calculated to investigate the mediating effects. We found that students' previous knowledge of English but not their AI knowledge directly affected their SRL with the chatbot, and that satisfying the need for autonomy and competence mediated the relationships between both knowledge (AI and English) and SRL, but relatedness did not. The self-directed nature of SRL requires heavy cognitive learning and satisfying the need for autonomy and competence may more effectively engage young children in this type of learning. The findings also revealed that current chatbot technologies may not benefit students with relatively lower levels of English proficiency. We suggest that teachers can use conversational chatbots for knowledge consolidation purposes, but not in SRL explorations. Practitioner notes What is already known about this topic Artificial intelligence (AI) technologies can potentially support students' self-regulated learning (SRL) of disciplinary knowledge through chatbots. Needs satisfaction in Self-determination theory (SDT) can explain the directive process required for SRL. Technical and disciplinary knowledge would affect SRL with technologies. What this paper adds This study examines the mediating effects of needs satisfaction in SDT on the relationship between students' previous AI (technical) and English (disciplinary) knowledge and SRL, using an AI conversational chatbot. Students' previous knowledge of English but not their AI knowledge directly affected their SRL with the chatbot. Autonomy and competence were mediators, but relatedness was not. Implications for practice and/or policy Teachers should use chatbots for knowledge consolidation rather than exploration. Teachers should support students' competence and autonomy, as these were found to be the factors that directly predicted SRL. School leaders and teacher educators should include the mediating effects of needs satisfaction in professional development programmes for digital education.},
  year     = {2023}
}

@misc{Peng23,
  title         = {The Impact of AI on Developer Productivity: Evidence from GitHub Copilot},
  author        = {Sida Peng and Eirini Kalliamvakou and Peter Cihon and Mert Demirer},
  year          = {2023},
  eprint        = {2302.06590},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/abs/2302.06590}
}

@misc{Pandey24,
  title         = {Transforming Software Development: Evaluating the Efficiency and Challenges of GitHub Copilot in Real-World Projects},
  author        = {Ruchika Pandey and Prabhat Singh and Raymond Wei and Shaila Shankar},
  year          = {2024},
  eprint        = {2406.17910},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/abs/2406.17910}
}

@inproceedings{Denny23,
  author    = {Denny, Paul and Kumar, Viraj and Giacaman, Nasser},
  title     = {Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language},
  year      = {2023},
  isbn      = {9781450394314},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3545945.3569823},
  doi       = {10.1145/3545945.3569823},
  abstract  = {GitHub Copilot is an artificial intelligence tool for automatically generating source code from natural language problem descriptions. Since June 2022, Copilot has officially been available for free to all students as a plug-in to development environments like Visual Studio Code. Prior work exploring OpenAI Codex, the underlying model that powers Copilot, has shown it performs well on typical CS1 problems thus raising concerns about its potential impact on how introductory programming courses are taught. However, little is known about the types of problems for which Copilot does not perform well, or about the natural language interactions that a student might have with Copilot when resolving errors. We explore these questions by evaluating the performance of Copilot on a publicly available dataset of 166 programming problems. We find that it successfully solves around half of these problems on its very first attempt, and that it solves 60\% of the remaining problems using only natural language changes to the problem description. We argue that this type of prompt engineering, which we believe will become a standard interaction between human and Copilot when it initially fails, is a potentially useful learning activity that promotes computational thinking skills, and is likely to change the nature of code writing skill development.},
  booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
  pages     = {1136–1142},
  numpages  = {7},
  keywords  = {artificial intelligence, cs1, foundation models, github copilot, introductory programming, large language models, openai},
  location  = {Toronto ON, Canada},
  series    = {SIGCSE 2023}
}

@article{Labadze23,
  author   = {Labadze, Lasha
              and Grigolia, Maya
              and Machaidze, Lela},
  title    = {Role of AI chatbots in education: systematic literature review},
  journal  = {International Journal of Educational Technology in Higher Education},
  year     = {2023},
  month    = {Oct},
  day      = {31},
  volume   = {20},
  number   = {1},
  pages    = {56},
  abstract = {AI chatbots shook the world not long ago with their potential to revolutionize education systems in a myriad of ways. AI chatbots can provide immediate support by answering questions, offering explanations, and providing additional resources. Chatbots can also act as virtual teaching assistants, supporting educators through various means. In this paper, we try to understand the full benefits of AI chatbots in education, their opportunities, challenges, potential limitations, concerns, and prospects of using AI chatbots in educational settings. We conducted an extensive search across various academic databases, and after applying specific predefined criteria, we selected a final set of 67 relevant studies for review. The research findings emphasize the numerous benefits of integrating AI chatbots in education, as seen from both students' and educators' perspectives. We found that students primarily gain from AI-powered chatbots in three key areas: homework and study assistance, a personalized learning experience, and the development of various skills. For educators, the main advantages are the time-saving assistance and improved pedagogy. However, our research also emphasizes significant challenges and critical factors that educators need to handle diligently. These include concerns related to AI applications such as reliability, accuracy, and ethical considerations.},
  issn     = {2365-9440},
  doi      = {10.1186/s41239-023-00426-1},
  url      = {https://doi.org/10.1186/s41239-023-00426-1}
}

@article{Callejo24,
  author  = {P. Callejo and C. Alario-Hoyos and C. Delgado-Kloos},
  title   = {Evaluating the Impact of ChatGPT on Programming Learning Outcomes in a Big Data Course},
  journal = {International Journal of Engineering Education},
  volume  = {40},
  number  = {4},
  pages   = {863--872},
  month   = {July},
  year    = {2024}
}


@article{Wang23,
  author     = {Wang, Cui-Yu and Gao, Bao-Lian and Chen, Shu-Jie},
  title      = {The effects of metacognitive scaffolding of project-based learning environments on students’ metacognitive ability and computational thinking},
  year       = {2023},
  issue_date = {Apr 2024},
  publisher  = {Kluwer Academic Publishers},
  address    = {USA},
  volume     = {29},
  number     = {5},
  issn       = {1360-2357},
  url        = {https://doi.org/10.1007/s10639-023-12022-x},
  doi        = {10.1007/s10639-023-12022-x},
  abstract   = {How to develop students' computational thinking (CT) is an important topic faced by academics and front-line teachers. However, the solution of programming problems requires paying attention to every detail of the problem and building a solution to the problem step by step, and for beginners, they often get stuck when one of these aspects goes wrong because of the lack of metacognitive abilities. The integration of metacognitive scaffolding in project-based programming instruction can help students identify their strengths, become more aware of their learning status and identify problems in a timely manner. Therefore, this study designed a metacognitive scaffolding in four aspects: planning, monitoring, reflecting and evaluating, and assessed the effects of this scaffolding on students' CT, learning achievement and metacognitive abilities through a quasi-experimental design. The participants were 70 students aged 9–11&nbsp;years in elementary school, where the experimental group (38 students) used a metacognitive scaffolding-based project-based learning approach, while the control group (32 students) used a traditional project-based learning approach. The results indicate that metacognitive scaffolding has a facilitative effect in helping students improve their CT and learning achievement, but does not significantly improve metacognitive abilities. This study provides insights into the deeper development of students' CT development and metacognitive scaffolding design.},
  journal    = {Education and Information Technologies},
  month      = jul,
  pages      = {5485–5508},
  numpages   = {24},
  keywords   = {Computational thinking, Metacognition, Metacognitive scaffolding, Computational thinking assessment, Problem-solving}
}


@article{Zheng19,
  title    = {The effects of group metacognitive scaffolding on group metacognitive behaviors, group performance, and cognitive load in computer-supported collaborative learning},
  journal  = {The Internet and Higher Education},
  volume   = {42},
  pages    = {13-24},
  year     = {2019},
  issn     = {1096-7516},
  doi      = {https://doi.org/10.1016/j.iheduc.2019.03.002},
  url      = {https://www.sciencedirect.com/science/article/pii/S1096751618303282},
  author   = {Lanqin Zheng and Xin Li and Xuan Zhang and Wei Sun},
  keywords = {Metacognitive scaffolding, Computer-supported collaborative learning, Metacognitive behavior, Group performance, Cognitive load},
  abstract = {Previous studies have reported the importance of metacognition on learning performance. However, little effort has been made to develop group metacognitive scaffolding (GMS) in computer-supported collaborative learning (CSCL) environments. The present study sought to extend current understandings of scaffolding in CSCL, by examining the effects of GMS on group metacognitive behavior, group performance, and cognitive load in CSCL environments. In total, 111 college students participated in the study. Participants were divided into small groups and assigned to experimental and control conditions. Students in the experimental group received GMS during an online collaborative learning process, while those in the control group performed online collaborative learning without GMS. The results indicated that GMS had significant impacts on group metacognitive behavioral transition and group performance. Moreover, GMS did not increase students' cognitive load. The results of this study and their implications for teachers and developers are discussed.}
}

@article{LiWei23,
  author   = {Li, Wei
              and Liu, Cheng-Ye
              and Tseng, Judy C. R.},
  title    = {Effects of the interaction between metacognition teaching and students' learning achievement on students' computational thinking, critical thinking, and metacognition in collaborative programming learning},
  journal  = {Education and Information Technologies},
  year     = {2023},
  month    = {Oct},
  day      = {01},
  volume   = {28},
  number   = {10},
  pages    = {12919-12943},
  abstract = {Collaborative programming can develop computational thinking and knowledge of computational programming. However, the researchers pointed out that because students often fail to mobilize metacognition to regulate and control their cognitive activities in a cooperation, this results in poor learning effects. Especially low-achieving students need more metacognitive support. Therefore, this study proposed a metacognition-based collaborative programming approach (M-CPA) to improve students' performance in collaborative programming. To evaluate the effectiveness of the method for students with different levels of learning achievement, a 7-week experiment was conducted. A total of 222 middle school students were divided into the experimental group with the M-CPA learning and the control group with the conventional collaborative programming approach (C-CPA). The results showed that learning methods and learning achievement had interactive effects on computational thinking tendency, critical thinking tendency, and metacognition tendency. M-CPA could significantly improve students' computational thinking tendency, critical thinking tendency, and metacognition tendency. Moreover, the proposed approach is more effective for low-achieving students. The results also showed that M-CPA could improve the students' achievement in program analysis questions.},
  issn     = {1573-7608},
  doi      = {10.1007/s10639-023-11671-2},
  url      = {https://doi.org/10.1007/s10639-023-11671-2}
}


@inproceedings{Margulieux24,
  author    = {Margulieux, Lauren E. and Prather, James and Reeves, Brent N. and Becker, Brett A. and Cetin Uzun, Gozde and Loksa, Dastyni and Leinonen, Juho and Denny, Paul},
  title     = {Self-Regulation, Self-Efficacy, and Fear of Failure Interactions with How Novices Use LLMs to Solve Programming Problems},
  year      = {2024},
  isbn      = {9798400706004},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi-org.ez348.periodicos.capes.gov.br/10.1145/3649217.3653621},
  doi       = {10.1145/3649217.3653621},
  abstract  = {We explored how undergraduate introductory programming students naturalistically used generative AI to solve programming problems. We focused on the relationship between their use of AI to their self-regulation strategies, self-efficacy, and fear of failure in programming. In this repeated-measures, mixed-methods research, we examined students' patterns of using generative AI with qualitative student reflections and their self-regulation, self-efficacy, and fear of failure with quantitative instruments at multiple times throughout the semester. We also explored the relationships among these variables to learner characteristics, perceived usefulness of AI, and performance. Overall, our results suggest that student factors affect their baseline use of AI. In particular, students with higher self-efficacy, lower fear of failure, or higher prior grades tended to use AI less or later in the problem-solving process and rated it as less useful than others. Interestingly, we found no relationship between students' self-regulation strategies and their use of AI. Students who used AI less or later in problem-solving also had higher grades in the course, but this is most likely due to prior characteristics as our data do not suggest that this is a causal relationship.},
  booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
  pages     = {276–282},
  numpages  = {7},
  keywords  = {CS1, LLMs, artificial intelligence, copilot, fear of failure, generative ai, introductory programming, large language models, metacognition, self-efficacy, self-regulated learning, self-regulation},
  location  = {Milan, Italy},
  series    = {ITiCSE 2024}
}

@article{Khusnul24,
  title        = {Enhancing Metacognitive and Creativity Skills through AI-Driven Meta-Learning Strategies},
  volume       = {18},
  url          = {https://online-journals.org/index.php/i-jim/article/view/47705},
  doi          = {10.3991/ijim.v18i05.47705},
  abstractnote = {&amp;lt;p&amp;gt;This study investigates the efficacy of a meta-learning approach in improving metacognitive and creative skills. This quantitative study focused on an experimental group using a onegroup pretest-posttest research design. All participants underwent a pretest to assess their initial metacognitive abilities and were subsequently exposed to a meta-learning framework throughout the course. A post-test was conducted to assess the impact of the intervention. The findings indicate a statistically significant improvement in metacognitive skills from the pretest to the post-test. This study confirms the effectiveness of meta-learning strategies and elucidates the relationship between meta-learning and metacognition. Meta-learning enables students to comprehend their own learning processes, thereby improving their capacity to strategize, oversee, and control their cognitive functions with the assistance of artificial intelligence (AI). This approach incorporates creative elements that can stimulate metacognitive thinking, encouraging students to adjust their learning strategies and think outside the box. This research suggests that meta-learning can improve metacognitive abilities, providing valuable insights into educational technology and course design in higher education settings. &amp;lt;/p&amp;gt;},
  number       = {05},
  journal      = {International Journal of Interactive Mobile Technologies (iJIM)},
  author       = {Khusnul Khotimah and Rusijono and Andi Mariono},
  year         = {2024},
  month        = {Mar.},
  pages        = {pp. 18–31}
}


@article{Boudouaia24,
  author   = {Sun, Dan
              and Boudouaia, Azzeddine
              and Zhu, Chengcong
              and Li, Yan},
  title    = {Would ChatGPT-facilitated programming mode impact college students' programming behaviors, performances, and perceptions? An empirical study},
  journal  = {International Journal of Educational Technology in Higher Education},
  year     = {2024},
  month    = {Feb},
  day      = {22},
  volume   = {21},
  number   = {1},
  pages    = {14},
  abstract = {ChatGPT, an AI-based chatbot with automatic code generation abilities, has shown its promise in improving the quality of programming education by providing learners with opportunities to better understand the principles of programming. However, limited empirical studies have explored the impact of ChatGPT on learners' programming processes. This study employed a quasi-experimental design to explore the possible impact of ChatGPT-facilitated programming mode on college students' programming behaviors, performances, and perceptions. 82 college students were randomly divided into two classes. One class employed ChatGPT-facilitated programming (CFP) practice and the other class utilized self-directed programming (SDP) mode. Mixed methods were utilized to collect multidimensional data. Data analysis uncovered some intriguing results. Firstly, students in the CFP mode had more frequent behaviors of debugging and receiving error messages, as well as pasting console messages on the website and reading feedback. At the same time, students in the CFP mode had more frequent behaviors of copying and pasting codes from ChatGPT and debugging, as well as pasting codes to ChatGPT and reading feedback from ChatGPT. Secondly, CFP practice would improve college students' programming performance, while the results indicated that there was no statistically significant difference between the students in CFP mode and the SDP mode. Thirdly, student interviews revealed three highly concerned themes from students' user experience about ChatGPT: the services offered by ChatGPT, the stages of ChatGPT usage, and experience with ChatGPT. Finally, college students' perceptions toward ChatGPT significantly changed after CFP practice, including its perceived usefulness, perceived ease of use, and intention to use. Based on these findings, the study proposes implications for future instructional design and the development of AI-powered tools like ChatGPT.},
  issn     = {2365-9440},
  doi      = {10.1186/s41239-024-00446-5},
  url      = {https://doi.org/10.1186/s41239-024-00446-5}
}

@misc{Eason23,
  title         = {GPTutor: a ChatGPT-powered programming tool for code explanation},
  author        = {Eason Chen and Ray Huang and Han-Shin Chen and Yuen-Hsien Tseng and Liang-Yi Li},
  year          = {2023},
  eprint        = {2305.01863},
  archiveprefix = {arXiv},
  primaryclass  = {cs.HC},
  url           = {https://arxiv.org/abs/2305.01863}
}








